# Описание задачи
На основе проектной деятельности ВУЗа нам была предложена работа над проектом "Разработка многомодальной системы распознавания эмоциональных состояний человека". В проекте участвовали: студентки 3 курса, учащиеся в НИУ ВШЭ, факультет ИМиКН, направление Прикладная математика и информатика Сидорова Анна Павловна и Дудникова Екатерина Олеговна. 

# Техническое задание:

- Изучить современные (state-of-the-art) статьи по задаче распознавания эмоций;
- Изучить и произвести статистический анализ открытого набора данных MELD;
- Изучить и применить методы классического обучения;
- Изучить и применить современные архитектуры нейронных сетей;
- Изучить и применить методы извлечения признаков из видеомадальности (сверточные нейронные сети);
- Изучить и применить методы извлечения признаков из аудиомодальности (mel-спектрограммы, toolkit openSMILE);
- Изучить и применить методы извлечения признаков из текстовой модальности (Вектора, на основе тональных словарей, BERT);
- Для каждой модальности построить классификатор и оценить качество;
- Объединить модальности на уровне векторов и построить общий классификатор, оценить качество;
- Объединить модальности на уровне принятия решений, т.е. Построить классификатор, принимающий решение на основе предсказаний локальных классификаторов, построенных на определенной модальности.
    

# Описание датасета
Мультимодальный набор данных Emotion Lines был создан путем улучшения и расширения набора данных Emotion Lines. MELD содержит те же экземпляры диалогов, что и в Emotion Lines, но он также включает в себя аудио и визуальные модальности наряду с текстом. MELD содержит более 1400 диалогов и 13000 высказываний из сериала "Друзья". 

# Выбранные метрики
- balanced accuracy
- accuracy
- f1 weighted

Однако метрики accuracy score и f1 weighted не являются показательными в данной задаче, так как их увеличение не гарантирует улучшение обобщаемости модели (а больше недообучение в класс neutral, что показано в аудио модели на основе признаков мел-спектрограмм). 

Данные метрики были взяты, чтобы сравнивать результаты с существующими лучшими моделями по многомодальным системам построенных на дате MELD. Поэтому основная метрика в этой работе - balanced accuracy.

# Распределение ролей:

**Анна Сидорова:**

- Унимодальная модель для аудио;
- Унимодальная модель (трансформеры) для текста;
- Объединение модальностей методом раннего слияния;
- Локальная библиотека для Telegram бота (Python);
- Трансформер speech-to-text для извлечения текстовой модальности из аудио (для бота).

**Екатерина Дудникова:**

На проекте Екатерина была ответственна за видео-модальность, а также частично за текст (классические NLP модели), работала над созданием многомодальной системы с помощью стратегии Late Fusion и в конце написала telegram бота, который и является итоговым продуктом нашего проекта.

# Назначение файлов:

В скобках файлов и некоторых папок указаны имена людей, которые работали над этой частью (Anna - Анна Сидорова, Ekaterina - Екатерина Дудникова).

**Папка reports**

В данной папке вы найдёте отчеты, которые поясняют суть и рассказывают об итогах проекта. В обоих файлах вы также найдете описание датасета. Советуется для прочтения в целях понимания материала.

- Отчет Анны ("Audio + Text + early fusion (Anna).pdf") рассказывает о её работе и результатах над Аудио, Текстом (трансформеры) и Early Fusion (+ немного об STT).
- Отчет Екатерины ("Video + Text + late fusion (Ekaterina).pdf") рассказывает о её работе и результатах над Видео, Текстом (классические NLP модели) и Late Fusion.

**Папка models**

**!В целях конфиденциальности некоторые части кода были сокрыты!**

- **Папка Unimodals**
  - **Папка Audio (Anna)** хранит в себе три блокнота .ipynb. Эти три блокнота описывают следующие шаги:
    - "first-step-analysis.ipynb.zip" блокнот с началом работы. Здесь анализируется датасет MELD со стороны аудио и на примере просматриваются аудио признаки (мел-спектрограммы, MFCC, гармоничные и ударные звуки и так далее).
    - "second-step-feature_creating.ipynb" блокнот с вторым шагом работы. Здесь создаются файлы с признаками, на которых будут обучаться модели.
    - "third-step-Unimodal_Audio.ipynb" блокнот с третьим шагом работы. Здесь исследуются модели для унимодальной системы аудио модальности. Изучены SVM и MLP для набора признаков, полученных с помощью toolkit openSMILE, CNN для мел-спектрограмм и мел-кепстральных коэффициентов.
    
   - **Папка Text (Anna + Ekaterina)**
      - **Папка Transformers (Anna)**
        - "fourth-step-Unimodal_Text_RoBERTa.ipynb" блокнот с четвертом шагом работы (для Анны). Здесь построена модель RoBERTa для текстовой модальности и выведены ее результаты.
      
      - **Папка Classic NLP (Ekaterina)** описания пока нет.
  
   - **Папка Video (Ekaterina)** описания пока нет.

- **Папка Multimodals**
  - **Папка early fusion (Anna)**
    - "fifth-step-get_probs_and_embs.ipynb" блокнот с пятым шагом работы (для Анны). Здесь были получены эмбеддинги из текстовой модальности для раннего слияния и вероятности классов для каждого аудио для позднего слияния.
    - "sixth-step-early_fusion.ipynb" блокнот с шестым шагом работы (для Анны). Здесь были построены многомодальные модели на основе раннего слияния.
    - "seventh-step-early_fusion_params.ipynb" блокнот с седьмым шагом работы (для Анны). Здесь была улучшена самая показательная модель раннего слияния путем подбора коэффициентов.

**Папка bot and biblio**
  - **Папка local_biblio (Anna)** в данной папке находятся файлы, написанные для локальной библиотеки. 
  - **Папка bot (Ekaterina)** описания пока нет.
 
